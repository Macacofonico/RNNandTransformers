{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1cf9259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e2f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f09cd135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the clusters\n",
    "\n",
    "def balance_df(df1, df2, random_state = 42):\n",
    "    df = (df1, df2)\n",
    "    lenght = (len(df[0]), len(df[1]))\n",
    "    idx = np.argmin([lenght[0], lenght[1]])\n",
    "    return pd.concat([df[idx], df[1-idx].sample(lenght[idx], random_state=random_state)], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14cbc551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove regulars expressions\n",
    "\n",
    "import re\n",
    "import string\n",
    "def custom_preprocessor(text):\n",
    "    '''\n",
    "    Make text lowercase, remove text in square brackets,remove links,remove special characters\n",
    "    and remove words containing numbers.\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = re.sub('u.s.', 'usa', text)\n",
    "    text = re.sub('can\\'t', 'cant', text)\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub(\"\\\\W\",\" \",text) # remove special chars\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce4071cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news = pd.read_csv('Fake.csv')\n",
    "true_news = pd.read_csv('True.csv')\n",
    "\n",
    "fake_news['target'] = 0\n",
    "true_news['target'] = 1\n",
    "\n",
    "news = balance_df(fake_news, true_news)\n",
    "news['text'] = news['text'].apply(custom_preprocessor)\n",
    "news['title'] = news['title'].apply(custom_preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "723e8293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature = 'title'\n",
    "target = 'target'\n",
    "\n",
    "X = news[feature]\n",
    "y = news[target]\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y,random_state=42,test_size=0.2)\n",
    "X_train,X_val,y_train,y_val = train_test_split(X_train, y_train,random_state=42,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b284c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature = 'title'\n",
    "target = 'target'\n",
    "\n",
    "X = news[[feature, target]]\n",
    "\n",
    "X_train, X_val = train_test_split(X,random_state=42,test_size=0.2)\n",
    "X_train,X_test = train_test_split(X_train,random_state=42,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87c30964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed set for future use.\n",
    "\n",
    "X_train.to_csv('train.csv', index=False)\n",
    "X_val.to_csv('validation.csv', index=False)\n",
    "X_test.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b293994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "857b1f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b735a9fe911e6231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/jovyan/.cache/huggingface/datasets/csv/default-b735a9fe911e6231/0.0.0/e138af468cb14e747fb46a19c787ffcfa5170c821476d20d5304287ce12bbc23...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/jovyan/.cache/huggingface/datasets/csv/default-b735a9fe911e6231/0.0.0/e138af468cb14e747fb46a19c787ffcfa5170c821476d20d5304287ce12bbc23. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = datasets.load_dataset('csv', data_files={'train': 'train.csv',\n",
    "                                                        'validation': 'validation.csv',\n",
    "                                                        'test': 'test.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af903bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comey to testify to senate panel in public session',\n",
       " 'u n  condemns anti gay crackdowns in egypt  azerbaijan  indonesia',\n",
       " ' watch  the daily show epically destroys fox news for blatant racism',\n",
       " 'trump scrambles to convince americans he can handle puerto rico crisis',\n",
       " 'turkey should follow west s lead on rights  author orhan pamuk',\n",
       " 'malaysia s ruling party unites behind najib as election looms',\n",
       " 'abc news reports  las vegas massacre suspect s hard drive is missing from his laptop',\n",
       " 'trumpdom  the curious world of trump s foreign policy explained',\n",
       " 'wow  what john kasich just asked cruz and trump to do proves he s got an ego the size of texas',\n",
       " 'breaking  trump hits back at rep john lewis who declared trump s presidency  illegitimate ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train']['title'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89922744",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82bbb060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"roberta-fake-news\"\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)#, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76a8dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98b4feb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a77995695442b888a49fc13157b821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4ac89fcff54afa9b68f73b8c8b9cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166201e964cd4900b26afb28032244c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[feature], padding='max_length', truncation=True, max_length = 50)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f18f6912",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comey to testify to senate panel in public session',\n",
       " 'u n  condemns anti gay crackdowns in egypt  azerbaijan  indonesia',\n",
       " ' watch  the daily show epically destroys fox news for blatant racism',\n",
       " 'trump scrambles to convince americans he can handle puerto rico crisis',\n",
       " 'turkey should follow west s lead on rights  author orhan pamuk',\n",
       " 'malaysia s ruling party unites behind najib as election looms',\n",
       " 'abc news reports  las vegas massacre suspect s hard drive is missing from his laptop',\n",
       " 'trumpdom  the curious world of trump s foreign policy explained',\n",
       " 'wow  what john kasich just asked cruz and trump to do proves he s got an ego the size of texas',\n",
       " 'breaking  trump hits back at rep john lewis who declared trump s presidency  illegitimate ']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train']['title'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c460c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94876b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85507dcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-19 20:05:56.764444: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-07-19 20:05:56.764495: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-07-19 20:05:58.026441: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-07-19 20:05:58.026483: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-07-19 20:05:58.026506: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (3383a395dced): /proc/driver/nvidia/version does not exist\n",
      "2021-07-19 20:05:58.026679: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-19 20:05:58.043784: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoConfig\n",
    "#config = AutoConfig.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57f951d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification at 0x7fe0d232e1f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f49a15df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'target', 'title'],\n",
       "    num_rows: 6854\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_dataset = tokenized_datasets['train']\n",
    "full_eval_dataset = tokenized_datasets['validation']\n",
    "full_test_dataset = tokenized_datasets['test']\n",
    "full_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53220f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42834"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_len = len(full_train_dataset) + len(full_eval_dataset) + len(full_test_dataset)\n",
    "total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3d25ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16001307372647897"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_test_dataset)/total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5539193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_RECORDS = len(full_test_dataset)# 4000\n",
    "SEED = 3456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67d5fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets['train'].shuffle(seed=SEED).select(range(round(len(full_train_dataset)/total_len*TOTAL_RECORDS)))\n",
    "eval_dataset = tokenized_datasets['validation'].shuffle(seed=SEED).select(range(round(len(full_eval_dataset)/total_len*TOTAL_RECORDS)))\n",
    "test_dataset = tokenized_datasets['test']#.shuffle(seed=SEED).select(range(round(len(full_test_dataset)/total_len*TOTAL_RECORDS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "851e1318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': <tf.Tensor: shape=(50,), dtype=int64, numpy=\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0])>,\n",
       " 'input_ids': <tf.Tensor: shape=(50,), dtype=int64, numpy=\n",
       " array([  101, 27885,  3630, 25171, 24247,  6398,  2003,  3718,  2013,\n",
       "         8398,  2811,  3034,  8398,  5176,  3036,  2000,  3288,  2032,\n",
       "         2067,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0])>,\n",
       " 'target': <tf.Tensor: shape=(), dtype=int64, numpy=0>}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.remove_columns([feature]).with_format(\"tensorflow\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fd3eac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': <tf.Tensor: shape=(50,), dtype=int64, numpy=\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0])>,\n",
       " 'input_ids': <tf.Tensor: shape=(50,), dtype=int64, numpy=\n",
       " array([  101,  5095,  2305,  2444,  1055,  2695, 17331, 16181,  2001,\n",
       "         3599,  1996,  4756,  2057,  2734,  2678,   102,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0])>,\n",
       " 'target': <tf.Tensor: shape=(), dtype=int64, numpy=0>}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.remove_columns([feature]).with_format(\"tensorflow\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfb834d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = train_dataset.remove_columns([feature]).with_format(\"tensorflow\")\n",
    "tf_eval_dataset = eval_dataset.remove_columns([feature]).with_format(\"tensorflow\")\n",
    "#tf_test_dataset = test_dataset.remove_columns([feature,\"target\"]).with_format(\"tensorflow\")\n",
    "tf_test_dataset = test_dataset.remove_columns([feature]).with_format(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f28d592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'target'],\n",
       "    num_rows: 4386\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3a03ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'target'],\n",
       "    num_rows: 6854\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e5a4d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02864a6a",
   "metadata": {},
   "source": [
    "Convert everything in big tensor and use from_tensor_slices method so that data can be fed into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a824d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = {x: tf_train_dataset[x].to_tensor() for x in tokenizer.model_input_names}\n",
    "train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features, tf_train_dataset[target]))\n",
    "train_tf_dataset = train_tf_dataset.shuffle(len(tf_train_dataset)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40e26b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_features = {x: tf_eval_dataset[x].to_tensor() for x in tokenizer.model_input_names}\n",
    "eval_tf_dataset = tf.data.Dataset.from_tensor_slices((eval_features, tf_eval_dataset[target]))\n",
    "eval_tf_dataset = eval_tf_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9438a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
    "    #loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    #metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=tf.keras.metrics.BinaryAccuracy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1cc53487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "pre_classifier (Dense)       multiple                  590592    \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 66,955,010\n",
      "Trainable params: 66,955,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94566880",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36e556d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "549/549 [==============================] - ETA: 0s - loss: 4.2415 - binary_accuracy: 0.5424WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "549/549 [==============================] - 388s 695ms/step - loss: 4.2415 - binary_accuracy: 0.5424 - val_loss: 0.6750 - val_binary_accuracy: 0.7024\n",
      "Epoch 2/5\n",
      "549/549 [==============================] - 387s 704ms/step - loss: 0.8727 - binary_accuracy: 0.7558 - val_loss: 0.4379 - val_binary_accuracy: 0.8333\n",
      "Epoch 3/5\n",
      "549/549 [==============================] - 380s 693ms/step - loss: 0.3827 - binary_accuracy: 0.8507 - val_loss: 0.3224 - val_binary_accuracy: 0.8782\n",
      "Epoch 4/5\n",
      "549/549 [==============================] - 385s 701ms/step - loss: 0.3266 - binary_accuracy: 0.8843 - val_loss: 0.3578 - val_binary_accuracy: 0.8931\n",
      "Epoch 5/5\n",
      "549/549 [==============================] - 377s 687ms/step - loss: 0.4701 - binary_accuracy: 0.8641 - val_loss: 0.2792 - val_binary_accuracy: 0.9063\n"
     ]
    }
   ],
   "source": [
    "if RUN_MODEL:\n",
    "    model.fit(train_tf_dataset, validation_data=eval_tf_dataset, epochs=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "adce3f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_MODEL:\n",
    "    model.save_pretrained(\"my_model_title_BinaryCrossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "234f4ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = {x: tf_test_dataset[x].to_tensor() for x in tokenizer.model_input_names}\n",
    "test_tf_dataset = tf.data.Dataset.from_tensor_slices((test_features, tf_test_dataset[target]))\n",
    "test_tf_dataset = test_tf_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3671de20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at my_model_title_BinaryCrossentropy were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at my_model_title_BinaryCrossentropy and are newly initialized: ['dropout_79']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "prediction_model_name = \"my_model_title_BinaryCrossentropy\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(prediction_model_name, num_labels = 2)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    #loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    #metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=tf.keras.metrics.BinaryAccuracy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd5495d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "857/857 [==============================] - 118s 136ms/step - loss: 0.2928 - binary_accuracy: 0.9003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2927630841732025, 0.9002771973609924]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.evaluate(test_tf_dataset)\n",
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
